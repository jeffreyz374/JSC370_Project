---
title: "JSC370 Assignment 4"
author: "Xiaotang (Jeffrey) Zhou"
date: "28/03/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(RCurl)
library(kableExtra)
library(parallel)
library(foreach)
library(doParallel)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(xgboost)
library(caret)
```

## Part 1: Rewriting R Functions to Make Them Faster

### Total row sums:
```{r}
fun1 <- function(mat) {
  n <- nrow(mat)
  ans <- double(n)
  for (i in 1:n) {
    ans[i] <- sum(mat[i, ])
  }
  ans
}

fun1alt <- function(mat) {
  rowSums(mat)
}
```

### Test to see if `fun1alt` is actually faster than `fun1`:
```{r, echo = FALSE}
set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)
m1 <- microbenchmark::microbenchmark(
  fun1(dat), fun1alt(dat), unit = "ms", check = "equivalent")

summary(m1) %>%
  kable(caption = "Summary statistics of the sampled runtimes (in milliseconds) for the total row sum functions",
        col.names = c("Function", "Minimum Runtime", "Lower Quartile of Runtime", 
                      "Average Runtime", "Median Runtime", "Upper Quartile of Runtime",
                      "Maximum Runtime", "Number of Evaluations")) %>%
  kable_styling()
```

We can see above that for every summary statistic (i.e. the minimum, 1st quartile, mean, median, 3rd quartile, and the maximum), the runtime in milliseconds for the alternate total row sum function is always less than the runtime for the original total row sum function. Thus, we have verified that the alternate total row sum function actually IS faster than the original total row sum function.

### Cumulative sum by row:
```{r}
fun2 <- function(mat) {
    n <- nrow(mat)
    k <- ncol(mat)
    ans <- mat
    for (i in 1:n) {
        for (j in 2:k) {
            ans[i,j] <- mat[i, j] + ans[i, j - 1]
        }
    }
    ans
}

fun2alt <- function(mat) {
  t(apply(mat, 1, cumsum))
}
```

### Test to see if `fun2alt` is actually faster than `fun2`:
```{r, echo = FALSE}
set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)
m2 <- microbenchmark::microbenchmark(
  fun2(dat), fun2alt(dat), unit = "ms", check = "equivalent")

summary(m2) %>%
  kable(caption = "Summary statistics of the sampled runtimes (in milliseconds) for the cumulative sum by row functions",
        col.names = c("Function", "Minimum Runtime", "Lower Quartile of Runtime", 
                      "Average Runtime", "Median Runtime", "Upper Quartile of Runtime",
                      "Maximum Runtime", "Number of Evaluations")) %>%
  kable_styling()
```

We can see above that for every summary statistic except the maximum, the runtime in milliseconds for the alternate cumulative sum by row function is always less than the runtime for the original cumulative sum by row function. Thus, we have verified that the alternate total row sum function actually IS faster than the original total row sum function.

### Simulating $\pi$:
```{r, warning = FALSE, message = FALSE}
sim_pi <- function(n = 1000, i = NULL) {
  p <- matrix(runif(n*2), ncol = 2)
  mean(rowSums(p^2) < 1) * 4
}

set.seed(1231)
system.time({
  ans1 <- unlist(lapply(1:4000, sim_pi, n = 10000))
  print(mean(ans1))
})

system.time({
  cl <- makePSOCKcluster(4L)
  clusterSetRNGStream(cl, 1231)
  clusterExport(cl, varlist = "sim_pi", envir = environment())
  ans2 <- unlist(parLapply(cl, 1:4000, sim_pi, n = 10000))
  print(mean(ans2))
  stopCluster(cl)
})
```

We can see above that there is a large difference between the `system.time` outputs for the simulation using `lapply` and the simulation using `parLapply` (with 4 CPUs) in that the second simulation was considerably faster than the first one, which is to be expected. Thus, we have verified that the simulation using simulation using `parLapply` actually IS faster than the simulation using `lapply`.

## Part 2: ML

```{r, echo = FALSE}
hitters <- getURL("https://raw.githubusercontent.com/JSC370/jsc370-2022/main/data/hitters/hitters.csv")
hitters <- read.csv(text = hitters)

set.seed(1005858859)
train_idx <- sample(1:nrow(hitters), floor(nrow(hitters) * 0.7))
test_idx <- setdiff(1:nrow(hitters), train_idx)
hitters_train <- hitters[train_idx,]
hitters_test <- hitters[test_idx,]
```


### Regression Tree:

First, consider the following regression tree, which we build as a result of fitting a regression tree model on the training part of the `hitters` dataset and prune based on the optimal complexity parameter:

```{r, echo = FALSE}
hitters_tree <- rpart(Salary ~ ., method = "anova", control = list(cp = 0), data = hitters_train)
optimalcp_reg <- hitters_tree$cptable[which.min(hitters_tree$cptable[,"xerror"]), "CP"]
hitters_tree_prune <- prune(hitters_tree, cp = optimalcp_reg)

rpart.plot(hitters_tree_prune)
```

By looking at this tree, we can first make the observation that for each split, a player that satisfies the condition for the split is considered to be worse than a player that does NOT satisfy the condition. Take for example the "CHits < 450" condition at the very first split: if a player satisfies this, that means in their entire career they have hit the ball in play less than 450 times, while a player who doesn't satisfy this has hit the ball in play at least 450 times in their career, making the latter player more valuable than the former based on the CHits feature. 

Using the previous observation and the general trend in the tree where the more times a player does NOT satisfy the split condition the greater their salary is predicted to be, we can conclude that this model predicts that players who hit the ball in play and bat runs in more than others are paid higher salaries than others, which is expected and makes sense.

Now, we will calculate the test MSE for this model:
```{r, echo = FALSE}
hitters_tree_pred <- predict(hitters_tree_prune, hitters_test)
hitters_test_tree <- cbind(hitters_test, hitters_tree_pred)

tree_mse <- sum((hitters_test_tree$hitters_tree_pred - hitters_test_tree$Salary)^2, na.rm = TRUE) / dim(hitters_test_tree)[1]
tree_mse
```

As seen above, the final test MSE for regression trees is `r toString(round(tree_mse, 2))`.

### Bagging:

Next, consider the following variable importance plot, which we derive as a result of fitting a bagging model on the training part of the `hitters` dataset:  

```{r, echo = FALSE}
hitters_bag <- randomForest(Salary~., data = hitters_train, mtry = ncol(hitters_train) - 1, na.action = na.omit)

varImpPlot(hitters_bag, n.var = ncol(hitters_train) - 1, col = "red", main = "The Variable Importance of Each Feature 
     in the Bagging Model")

```

By looking at this variable importance plot, we can make the observation that the feature in the bagging model that is the "purest" is "CHmRun", which is the count of how many home runs a player hit over the course of his career. In other words, the number of home runs a player hit over the course of his career appears to be the best split on what his predicted salary will be in a bagging model, which is unsurprising as a player who hits more home runs will be obviously more valuable than a player that hits less home runs.

We can also make the overall observation that there don't appear to be any considerable gaps between dots in the plot, which means there aren't any big jumps between one feature's importance and another feature's importance. This could possibly be interpreted to mean that each feature's importance is more evenly balanced in this model, which could be a by-product of the model's construction where only random subsets of the training dataset are used to train the model while the number of features is held constant each time.

Now, we will calculate the test MSE for this model:
```{r, echo = FALSE}
hitters_bag_pred <- predict(hitters_bag, hitters_test)
hitters_test_bag <- cbind(hitters_test, hitters_bag_pred)

bag_mse <- sum((hitters_test_bag$hitters_bag_pred - hitters_test_bag$Salary)^2, na.rm = TRUE) / dim(hitters_test_bag)[1]
bag_mse
```

As seen above, the final test MSE for bagging is `r toString(round(bag_mse, 2))`.

### Random Forest:

Next, consider the following variable importance plot, which we derive as a result of fitting a random forest model on the training part of the `hitters` dataset:  

```{r, echo = FALSE}
hitters_rf <- randomForest(Salary~., data = hitters_train, na.action = na.omit)

varImpPlot(hitters_rf, n.var = ncol(hitters_train) - 1, col = "red", main = "The Variable Importance of Each Feature 
     in the Random Forest Model")
```

By looking at this variable importance plot, we can make the observation that like in the bagging model, the feature in the random forest model that is the "purest" is "CHmRun", except this time the purity value, which is recorded to be just over $4 \times 10^6$, seems to be less than the purity value in the bagging model, which was just over $5 \times 10^6$. In other words, the number of home runs a player hit over the course of his career is still the best split on what his predicted salary will be in a random forest model, although the importance appears to decrease. 

Furthermore, we can make an overall observation that unlike with the variable importance plot for the bagging model, there DO appear to be considerable gaps between dots in the plot for the random forest model. That is, in this variable importance plot, we can identify 3 clear clusters of dots with considerable distance separating them, with a particularly big gap separating the features of "Walks", the number of times the player was walked (put on base by the pitcher after being thrown 4 pitches outside the strike zone) in the year 1986, and "CRBI", the number of runs batted in by a player over the course of his career. This could possibly be explained by the fact that unlike with bagging where the model construction only involves training on random subsets of the training dataset, the random forest model's construction also involves training on random subsets of the features, which is how some features might have been given a greater influence over how to predict the salaries.

Now, we will calculate the test MSE for this model:
```{r, echo = FALSE}
hitters_rf_pred <- predict(hitters_rf, hitters_test)
hitters_test_rf <- cbind(hitters_test, hitters_rf_pred)

rf_mse <- sum((hitters_test_rf$hitters_rf_pred - hitters_test_rf$Salary)^2, na.rm = TRUE) / dim(hitters_test_rf)[1]
rf_mse
```

As seen above, the final test MSE for random forest is `r toString(round(rf_mse, 2))`.

### Boosting:

Next, consider the following plot depicting the relationship between various shrinkage parameters and test MSE, which we draw by fitting a boosting model on the training part of the `hitters` dataset with various values of $\lambda$ (the shrinkage parameter), finding the corresponding test MSE, and then plotting the results:  

```{r, echo = FALSE}
hitters$League_num <- as.factor(hitters$League)
hitters$Division_num <- as.factor(hitters$Division)
hitters$NewLeague_num <- as.factor(hitters$NewLeague)
hitters_num <- hitters %>% 
  select(-c(League, Division, NewLeague))
hitters_num <- na.omit(hitters_num)

hitters_train_num <- sample(1:nrow(hitters_num), floor(nrow(hitters_num) * 0.7))
hitters_test_num <- setdiff(1:nrow(hitters_num), hitters_train_num)

shrinkages <- c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1)
test_boost_mses <- rep(0, 7)
for (i in 1:7) {
  hitters_boost <- gbm(Salary~., data = hitters_num[hitters_train_num,], 
                       distribution = "gaussian", n.trees = 1000, shrinkage = shrinkages[i],
                       interaction.depth = 1, cv.folds = 10)
  salary_boost <- predict(hitters_boost, 
                          newdata = hitters_num[hitters_test_num,], 
                          n.trees = 1000)
  test_boost_mses[i] <- sum((salary_boost - hitters_num[hitters_test_num, "Salary"])^2) / dim(hitters_test_rf)[1]
}

plot(x = shrinkages, y = test_boost_mses, xlab = "Shrinkage Parameter", 
     ylab = "Mean Squared Error", 
     main = "The Relationship Between Different Shrinkage Parameter 
     Values and Test Mean Squared Error")
lines(x = shrinkages, y = test_boost_mses, col = 'blue')
```

By looking at the relationship between $\lambda$ values and test MSE, we can make the observation that the shrinkage parameter cannot be too small or too large if our goal is to minimize the test MSE. This is consistent with what we know about learning rates both for boosting and for the commonly used gradient descent algorithm, as a learning rate that is too small will likely be too slow to reach the optimal answer while a learning rate that is too large will just overshoot the optimal answer altogether. This is exactly what happened here, where the shrinkage parameter of 0.001 (the smallest shrinkage parameter we tested) was too slow and ended up not adequately running the algorithm which resulted in a high test MSE, and also where the shrinkage parameters of 0.5 and 1 (the 2 largest shrinkage parameters we tested) overshot the targeted optimal answer by so much that the models they built wound up producing the largest test MSEs.

Now, we will choose the shrinkage parameter that produced the lowest test MSE:

```{r, echo = FALSE}
boost_mse <- test_boost_mses[which.min(test_boost_mses)]
best_shrinkage <- shrinkages[which.min(test_boost_mses)]
best_shrinkage
boost_mse
```

As seen above, the shrinkage parameter that produced the lowest test MSE is `r toString(best_shrinkage)` with a test MSE of `r toString(round(boost_mse, 2))`. Using this shrinkage parameter, we can draw the following variable importance plot for the boosting model:

```{r, echo = FALSE}
hitters_boost_best <- gbm(Salary~., data = hitters_num[hitters_train_num,], 
                       distribution = "gaussian", n.trees = 1000, shrinkage = best_shrinkage,
                       interaction.depth = 1, cv.folds = 10)

var_imp_boost <- summary(hitters_boost_best)
rownames(var_imp_boost) <- NULL 
var_imp_boost %>%
  kable(caption = "Summary of the Relative Influence of Each Feature in the Boosting Model",
        col.names = c("Feature", "Relative Influence")) %>%
  kable_styling()
```

By using the shrinkage parameter that produced the best test MSE to draw the above variable importance plot and table, we can make the surprising observation that unlike either the bagging or random forest models, the feature in the boosting model that has the greatest relative influence in predicting a player's salary is "Walks", which is the number of times the player was walked (put on base by the pitcher after being thrown 4 pitches outside the strike zone) in the year 1986. This observation is surprising because not only does it imply that the number of times a player gets on base without hitting the ball is the most important feature in predicting a player's salary, but it also implies that this statistic measured just in 1986 somehow is more influential in the prediction than the same statistic measured over a player's whole career. 

Furthermore, we can make an overall observation that similar to the variable importance plot for the random forest model, the relative influence seems to be distributed in a very "top-heavy" fashion in that the 3 features with the highest relative influence ("Walks", "CRBI", "CHmRun") have relative influence 17.89, 15.96, and 14.84, while "Hits", the feature with the 4th highest relative influence, has a considerably lower relative influence of 9.85. This could possibly be explained by the fact that with boosting, a feature that is found to be better at the prediction task is given a greater amount of the prediction power that was taken away from a feature that is weaker, resulting in the top few most powerful features getting the greatest share of the relative influence over predicting the salaries while the other features get less of it.

### Extreme Gradient Boosting:

Finally, consider the following variable importance plot, which we derive as a result of fitting an extreme gradient boosting model with a grid search on $\eta$ on the training part of the `hitters` dataset:  

```{r, warning = FALSE, message = FALSE, include = FALSE}
train_control = trainControl(method = "cv", number = 10, search = "grid")

tune_grid <- expand.grid(max_depth = c(1, 3, 5, 7), nrounds = (1:10) * 50, 
                        eta = c(0.01, 0.1, 0.3), gamma = 0, subsample = 1,
                        min_child_weight = 1, colsample_bytree = 0.6)

hitters_xgb <- caret::train(Salary~., data = hitters_num[hitters_train_num,], 
                            method = "xgbTree", trControl = train_control, 
                            tuneGrid = tune_grid)
```

```{r, echo = FALSE}
plot(varImp(hitters_xgb, scale = FALSE), ylab = "Feature", main = "The Variable Importance of Each Feature 
     in the Extreme Gradient Boosting Model")
```

By looking at the above variable importance plot and table, we can make the more intuitive observation that the feature in the extreme gradient boosting model that has the greatest importance in predicting a player's salary is "CRBI", which is the number of runs batted in by a player over the course of his entire career. This is less surprising than the observation made for the boosting model because a player who bats in a greater number of runs and helps his team earn points is intuitively paid a higher salary than a player who bats in less runs. 

Furthermore, we can make an overall observation that similar to the variable importance plot for the boosting model, the variable importance seems to be distributed in a very "top-heavy" fashion, only this time the disparity looks to be even more extreme with "CRBI", the feature with the greatest importance, having a variable importance value of 0.285 (more than a quarter of the total importance) while the remaining 75% of the importance is divided up by every other feature. This large disparity could possibly be explained by the fact that extreme gradient boosting takes the properties of boosting to the extreme, which results in the top few most powerful features getting even more of the share of the importance over predicting the salaries than in boosting while the other features get even less of it.

Now, we will calculate the test MSE for this model:

```{r, echo = FALSE}
salary_xgb <- predict(hitters_xgb, newdata = hitters_num[hitters_test_num,])
xgb_mse <- (caret::RMSE(hitters_num[hitters_test_num, "Salary"], salary_xgb)) ** 2
xgb_mse
```

As seen above, the final test MSE for extreme gradient boosting is `r toString(round(xgb_mse, 2))`.

### Summary of Test MSEs and Comparisons Between Models:

All in all, we have the following test MSEs for the 5 models that we fit above:
```{r, echo = FALSE}
mse_table <- data.frame("Regression Tree MSE" = c(tree_mse),
                        "Bagging MSE" = c(bag_mse),
                        "Random Forest MSE" = c(rf_mse),
                        "Boosting MSE" = c(boost_mse),
                        "XGBoost MSE" = c(xgb_mse))

mse_table %>%
  kable(caption = "Summary of the test MSEs for each model",
        col.names = c("Regression Tree MSE", "Bagging MSE", "Random Forest MSE",
                      "Boosting MSE", "XGBoost MSE")) %>%
  kable_styling()
```

Using the numbers displayed in this table, we can see that the boosting model (with shrinkage parameter $\lambda =$ `r toString(best_shrinkage)`) produces the lowest test MSE, followed by extreme gradient boosting, random forest, bagging, and regression tree.

First, it is not too surprising that the regression tree model has the largest test MSE, as in that model we are trying to fit a linear regression on a set of data (the salaries of the players) that is more likely to follow a Gaussian pattern than a linear pattern. 

Next, as an attempt to correct the shortcomings of the regression tree model, the bagging and random forest models were constructed, with the former being built using random subsets of the training dataset and the latter also being built using random subsets of the training dataset but with the addition of using random subsets of the features too, both with the goal of moving past a model built off of linear regression. Surprisingly, their test MSEs don't differ by too much, which is strange because it was expected that the random forest property of also training on random subsets of features would correct the possible issue of some features being unfairly overrepresented over others.

Finally, in an effort to further improve the test MSEs of the bagging and random forest models, we built the boosting and extreme gradient boosting models. In these, we attempted to correct the assumption used in the regression tree model that the salaries are linearly distributed while also moving past the notion of training the model on random subsets of the dataset and the features by actually testing out different subsets of the dataset and features and optimizing using gradient descent. One thing that happened as a result was that in both the boosting and extreme gradient boosting models, a select few of the features were given a much greater share of the prediction power than the rest of the features as seen in the boosting model with "Walks", "CRBI", and "CHmRun" and in the extreme gradient boosting model with "CRBI". This can be used to explain the difference in the test MSEs of the boosting and extreme gradient boosting models in that by putting so much prediction power on just one feature as opposed to three, the extreme gradient boosting model was prone to making more errors than its boosting counterpart, which likely resulted in the boosting model having a slightly lower test MSE than the extreme gradient boosting model.

In conclusion, if we were to choose a model solely based on which one has the lowest test MSE, we would choose the boosting model. However, it is important to remember that as we saw earlier, the boosting model appears to give a large amount of the salary prediction power to the feature of "Walks", which is the number of times the player was walked (put on base by the pitcher after being thrown 4 pitches outside the strike zone) in the year 1986. This raises questions about whether this model is actually generalizable, as it would probably not be very useful to predict the salaries of players who played outside of the year 1986. With this, we could choose the extreme gradient boosting model instead, as it gives more predicting power to the feature of "CRBI", the number of runs batted in by a player over the course of his career (which is more generalizable because there is often a relationship between how well a player has played throughout his career and his salary), despite the slightly higher test MSE.